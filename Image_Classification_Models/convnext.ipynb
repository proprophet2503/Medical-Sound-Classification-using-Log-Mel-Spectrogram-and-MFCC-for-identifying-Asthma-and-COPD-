{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a363df29",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7543cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \n",
    "                      \"numpy<2.0.0\", \"timm\", \"albumentations\", \"optuna\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449ce85e",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f96dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "import timm\n",
    "from timm.loss import LabelSmoothingCrossEntropy\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import optuna\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228d1541",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab24dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    BASE_DIR = Path(\"/kaggle/input/logmel-cough-vowel\")\n",
    "    DATA_PATH = BASE_DIR / \"dataclean_1\"\n",
    "    CSV_PATH = BASE_DIR / \"logMelDataset\"\n",
    "    \n",
    "    COUGH_PATH = DATA_PATH / \"dataclean_cough_log_mel_1\"\n",
    "    VOWEL_PATH = DATA_PATH / \"dataclean_vowel_log_mel_1\"\n",
    "    TRAIN_CSV = CSV_PATH / \"train.csv\"\n",
    "    TEST_CSV = CSV_PATH / \"test.csv\"\n",
    "    OUTPUT_PATH = Path(\"/kaggle/working\")\n",
    "    \n",
    "    MODEL_NAME = \"convnext_tiny.fb_in22k_ft_in1k\"\n",
    "    IMG_SIZE = 224\n",
    "    NUM_CLASSES = 3\n",
    "    FALLBACK_CLASS = 0\n",
    "    \n",
    "    EPOCHS = 20\n",
    "    NUM_WORKERS = 2\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    SEED = 42\n",
    "    \n",
    "    N_TRIALS = 10\n",
    "    OPTUNA_EPOCHS = 5\n",
    "\n",
    "    @staticmethod\n",
    "    def create_output_dir():\n",
    "        Config.OUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "set_seed(Config.SEED)\n",
    "Config.create_output_dir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a726afa3",
   "metadata": {},
   "source": [
    "## Image Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7acfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(data='train'):\n",
    "    if data == 'train':\n",
    "        return A.Compose([\n",
    "            A.Resize(Config.IMG_SIZE, Config.IMG_SIZE),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "    else:\n",
    "        return A.Compose([\n",
    "            A.Resize(Config.IMG_SIZE, Config.IMG_SIZE),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2(),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1d6b24",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae330f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectrogramDataset(Dataset):\n",
    "    def __init__(self, df, img_dir, transform=None, is_test=False):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.img_dir = Path(img_dir)\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            row = self.df.iloc[idx]\n",
    "            img_id = row['id'] if 'id' in row else row['candidateID']\n",
    "            img_path = self.img_dir / img_id / \"log_mel_spectrogram.png\"\n",
    "            \n",
    "            if not img_path.exists(): \n",
    "                return None\n",
    "            \n",
    "            img = cv2.imread(str(img_path))\n",
    "            if img is None: \n",
    "                return None\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            if self.transform:\n",
    "                augmented = self.transform(image=img)\n",
    "                img = augmented['image']\n",
    "            \n",
    "            if self.is_test:\n",
    "                return img, img_id\n",
    "            else:\n",
    "                label = int(row['label'])\n",
    "                return img, label, img_id\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    if len(batch) == 0: \n",
    "        return None\n",
    "    return torch.utils.data.dataloader.default_collate(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d040d5d",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41342243",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNeXtTinyClassifier(nn.Module):\n",
    "    def __init__(self, model_name=Config.MODEL_NAME, num_classes=Config.NUM_CLASSES):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_name, pretrained=True, num_classes=0)\n",
    "        self.num_features = self.model.num_features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(self.num_features),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(self.num_features, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.model(x)\n",
    "        logits = self.classifier(features)\n",
    "        return logits, features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db79831f",
   "metadata": {},
   "source": [
    "## Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f6ea93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, scaler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "    \n",
    "    for batch in loader:\n",
    "        if batch is None: \n",
    "            continue\n",
    "        imgs, labels, _ = batch\n",
    "        imgs, labels = imgs.to(Config.DEVICE), labels.to(Config.DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            logits, _ = model(imgs)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    if len(all_preds) == 0:\n",
    "        return 0, 0\n",
    "        \n",
    "    return total_loss / len(loader), f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "\n",
    "def validate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            if batch is None: \n",
    "                continue\n",
    "            imgs, labels, _ = batch\n",
    "            imgs, labels = imgs.to(Config.DEVICE), labels.to(Config.DEVICE)\n",
    "            \n",
    "            with autocast():\n",
    "                logits, _ = model(imgs)\n",
    "                loss = criterion(logits, labels)\n",
    "                \n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    if len(all_preds) == 0:\n",
    "        return 0, 0, [], []\n",
    "        \n",
    "    return total_loss / len(loader), f1_score(all_labels, all_preds, average='macro', zero_division=0), all_preds, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ce4d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pipeline(model, train_loader, val_loader, params, save_name):\n",
    "    criterion = LabelSmoothingCrossEntropy(smoothing=0.1)\n",
    "    optimizer = AdamW(model.parameters(), lr=params['lr'], weight_decay=0.05)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=Config.EPOCHS)\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    best_f1 = 0\n",
    "    history = {'train_loss': [], 'val_f1': []}\n",
    "    \n",
    "    for epoch in range(Config.EPOCHS):\n",
    "        t_loss, t_f1 = train_epoch(model, train_loader, criterion, optimizer, scaler)\n",
    "        v_loss, v_f1, _, _ = validate(model, val_loader, criterion)\n",
    "        \n",
    "        history['train_loss'].append(t_loss)\n",
    "        history['val_f1'].append(v_f1)\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"[{save_name.upper()}] Epoch {epoch+1}/{Config.EPOCHS} | Loss: {t_loss:.4f} | Val F1: {v_f1:.4f}\")\n",
    "        \n",
    "        if v_f1 > best_f1:\n",
    "            best_f1 = v_f1\n",
    "            torch.save(model.state_dict(), Config.OUTPUT_PATH / f\"{save_name}_best.pth\")\n",
    "            \n",
    "        scheduler.step()\n",
    "    \n",
    "    return history, best_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c926ceec",
   "metadata": {},
   "source": [
    "## Optuna Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59136c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, train_df, img_dir):\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-3, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32])\n",
    "    \n",
    "    tr_df, val_df = train_test_split(train_df, test_size=0.2, stratify=train_df['label'], random_state=Config.SEED)\n",
    "    \n",
    "    tr_loader = DataLoader(\n",
    "        SpectrogramDataset(tr_df, img_dir, get_transforms('train')), \n",
    "        batch_size=batch_size, shuffle=True, num_workers=Config.NUM_WORKERS, \n",
    "        collate_fn=collate_fn, pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        SpectrogramDataset(val_df, img_dir, get_transforms('valid')), \n",
    "        batch_size=batch_size, shuffle=False, num_workers=Config.NUM_WORKERS, \n",
    "        collate_fn=collate_fn, pin_memory=True\n",
    "    )\n",
    "    \n",
    "    model = ConvNeXtTinyClassifier().to(Config.DEVICE)\n",
    "    criterion = LabelSmoothingCrossEntropy(smoothing=0.1)\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    best_temp_f1 = 0\n",
    "    for epoch in range(Config.OPTUNA_EPOCHS):\n",
    "        train_epoch(model, tr_loader, criterion, optimizer, scaler)\n",
    "        _, f1, _, _ = validate(model, val_loader, criterion)\n",
    "        best_temp_f1 = max(best_temp_f1, f1)\n",
    "        trial.report(f1, epoch)\n",
    "        if trial.should_prune(): \n",
    "            raise optuna.TrialPruned()\n",
    "        \n",
    "    del model, optimizer, scaler\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return best_temp_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff735789",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62047fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(Config.TRAIN_CSV)\n",
    "test_df = pd.read_csv(Config.TEST_CSV)\n",
    "\n",
    "train_df = train_df.rename(columns={'disease': 'label', 'candidateID': 'id'})\n",
    "\n",
    "cough_files = set(p.name for p in Config.COUGH_PATH.glob(\"*\") if p.is_dir())\n",
    "vowel_files = set(p.name for p in Config.VOWEL_PATH.glob(\"*\") if p.is_dir())\n",
    "\n",
    "train_df['has_cough'] = train_df['id'].isin(cough_files)\n",
    "train_df['has_vowel'] = train_df['id'].isin(vowel_files)\n",
    "\n",
    "print(f\"Train: {len(train_df)} | Cough: {train_df['has_cough'].sum()} | Vowel: {train_df['has_vowel'].sum()}\")\n",
    "print(f\"Test: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9659ec",
   "metadata": {},
   "source": [
    "## Train Cough Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546bba12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c = train_df[train_df['has_cough']].copy()\n",
    "\n",
    "study_c = optuna.create_study(direction='maximize', pruner=optuna.pruners.MedianPruner())\n",
    "study_c.optimize(lambda t: objective(t, df_c, Config.COUGH_PATH), n_trials=Config.N_TRIALS, show_progress_bar=True)\n",
    "params_c = study_c.best_params\n",
    "\n",
    "print(f\"Best Params: {params_c} | F1: {study_c.best_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0e456d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_c, val_c = train_test_split(df_c, test_size=0.2, stratify=df_c['label'], random_state=Config.SEED)\n",
    "model_c = ConvNeXtTinyClassifier().to(Config.DEVICE)\n",
    "\n",
    "hist_c, best_f1_c = train_pipeline(\n",
    "    model_c,\n",
    "    DataLoader(SpectrogramDataset(tr_c, Config.COUGH_PATH, get_transforms('train')), \n",
    "               batch_size=params_c['batch_size'], shuffle=True, collate_fn=collate_fn, pin_memory=True),\n",
    "    DataLoader(SpectrogramDataset(val_c, Config.COUGH_PATH, get_transforms('valid')), \n",
    "               batch_size=params_c['batch_size'], collate_fn=collate_fn, pin_memory=True),\n",
    "    params_c, \"cough\"\n",
    ")\n",
    "\n",
    "print(f\"Cough Model Best F1: {best_f1_c:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d82dfb",
   "metadata": {},
   "source": [
    "### Cough Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd74fdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_c.load_state_dict(torch.load(Config.OUTPUT_PATH / \"cough_best.pth\"))\n",
    "model_c.eval()\n",
    "\n",
    "val_c_loader = DataLoader(\n",
    "    SpectrogramDataset(val_c, Config.COUGH_PATH, get_transforms('valid')), \n",
    "    batch_size=32, shuffle=False, num_workers=2, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "criterion = LabelSmoothingCrossEntropy(smoothing=0.1)\n",
    "val_loss, val_f1, val_preds, val_labels = validate(model_c, val_c_loader, criterion)\n",
    "\n",
    "val_acc = accuracy_score(val_labels, val_preds)\n",
    "class_f1 = f1_score(val_labels, val_preds, average=None, zero_division=0)\n",
    "\n",
    "print(f\"Accuracy: {val_acc:.4f} | F1 Macro: {val_f1:.4f}\")\n",
    "print(f\"Per-Class F1: {class_f1}\")\n",
    "print(f\"\\n{classification_report(val_labels, val_preds, target_names=['Class 0', 'Class 1', 'Class 2'], zero_division=0)}\")\n",
    "\n",
    "cm_c = confusion_matrix(val_labels, val_preds)\n",
    "fig_cm_c, ax_cm_c = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(cm_c, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Class 0', 'Class 1', 'Class 2'],\n",
    "            yticklabels=['Class 0', 'Class 1', 'Class 2'],\n",
    "            ax=ax_cm_c, cbar_kws={'label': 'Count'})\n",
    "ax_cm_c.set_xlabel('Predicted', fontsize=12, fontweight='bold')\n",
    "ax_cm_c.set_ylabel('True', fontsize=12, fontweight='bold')\n",
    "ax_cm_c.set_title('Cough Model - Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(Config.OUTPUT_PATH / \"cough_confusion_matrix.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245c6b36",
   "metadata": {},
   "source": [
    "## Train Vowel Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd741a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_v = train_df[train_df['has_vowel']].copy()\n",
    "\n",
    "study_v = optuna.create_study(direction='maximize', pruner=optuna.pruners.MedianPruner())\n",
    "study_v.optimize(lambda t: objective(t, df_v, Config.VOWEL_PATH), n_trials=Config.N_TRIALS, show_progress_bar=True)\n",
    "params_v = study_v.best_params\n",
    "\n",
    "print(f\"Best Params: {params_v} | F1: {study_v.best_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c8fce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_v, val_v = train_test_split(df_v, test_size=0.2, stratify=df_v['label'], random_state=Config.SEED)\n",
    "model_v = ConvNeXtTinyClassifier().to(Config.DEVICE)\n",
    "\n",
    "hist_v, best_f1_v = train_pipeline(\n",
    "    model_v,\n",
    "    DataLoader(SpectrogramDataset(tr_v, Config.VOWEL_PATH, get_transforms('train')), \n",
    "               batch_size=params_v['batch_size'], shuffle=True, collate_fn=collate_fn, pin_memory=True),\n",
    "    DataLoader(SpectrogramDataset(val_v, Config.VOWEL_PATH, get_transforms('valid')), \n",
    "               batch_size=params_v['batch_size'], collate_fn=collate_fn, pin_memory=True),\n",
    "    params_v, \"vowel\"\n",
    ")\n",
    "\n",
    "print(f\"Vowel Model Best F1: {best_f1_v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c23cf8b",
   "metadata": {},
   "source": [
    "### Vowel Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c90913",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_v.load_state_dict(torch.load(Config.OUTPUT_PATH / \"vowel_best.pth\"))\n",
    "model_v.eval()\n",
    "\n",
    "val_v_loader = DataLoader(\n",
    "    SpectrogramDataset(val_v, Config.VOWEL_PATH, get_transforms('valid')), \n",
    "    batch_size=32, shuffle=False, num_workers=2, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loss_v, val_f1_v, val_preds_v, val_labels_v = validate(model_v, val_v_loader, criterion)\n",
    "\n",
    "val_acc_v = accuracy_score(val_labels_v, val_preds_v)\n",
    "class_f1_v = f1_score(val_labels_v, val_preds_v, average=None, zero_division=0)\n",
    "\n",
    "print(f\"Accuracy: {val_acc_v:.4f} | F1 Macro: {val_f1_v:.4f}\")\n",
    "print(f\"Per-Class F1: {class_f1_v}\")\n",
    "print(f\"\\n{classification_report(val_labels_v, val_preds_v, target_names=['Class 0', 'Class 1', 'Class 2'], zero_division=0)}\")\n",
    "\n",
    "cm_v = confusion_matrix(val_labels_v, val_preds_v)\n",
    "fig_cm_v, ax_cm_v = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(cm_v, annot=True, fmt='d', cmap='Greens', \n",
    "            xticklabels=['Class 0', 'Class 1', 'Class 2'],\n",
    "            yticklabels=['Class 0', 'Class 1', 'Class 2'],\n",
    "            ax=ax_cm_v, cbar_kws={'label': 'Count'})\n",
    "ax_cm_v.set_xlabel('Predicted', fontsize=12, fontweight='bold')\n",
    "ax_cm_v.set_ylabel('True', fontsize=12, fontweight='bold')\n",
    "ax_cm_v.set_title('Vowel Model - Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(Config.OUTPUT_PATH / \"vowel_confusion_matrix.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0071afed",
   "metadata": {},
   "source": [
    "## Training Metrics Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca66b446",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax[0].plot(hist_c['val_f1'], marker='o', label=f'Cough (Best: {best_f1_c:.3f})', linewidth=2)\n",
    "ax[0].plot(hist_v['val_f1'], marker='s', label=f'Vowel (Best: {best_f1_v:.3f})', linewidth=2)\n",
    "ax[0].set_xlabel('Epoch', fontsize=12)\n",
    "ax[0].set_ylabel('Validation F1 Score', fontsize=12)\n",
    "ax[0].set_title('Model Performance', fontsize=14, fontweight='bold')\n",
    "ax[0].legend(fontsize=10)\n",
    "ax[0].grid(True, alpha=0.3)\n",
    "\n",
    "ax[1].plot(hist_c['train_loss'], marker='o', label='Cough', linewidth=2)\n",
    "ax[1].plot(hist_v['train_loss'], marker='s', label='Vowel', linewidth=2)\n",
    "ax[1].set_xlabel('Epoch', fontsize=12)\n",
    "ax[1].set_ylabel('Training Loss', fontsize=12)\n",
    "ax[1].set_title('Training Loss', fontsize=14, fontweight='bold')\n",
    "ax[1].legend(fontsize=10)\n",
    "ax[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(Config.OUTPUT_PATH / \"training_metrics.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1856c78b",
   "metadata": {},
   "source": [
    "## Generate Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6241d40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_c.load_state_dict(torch.load(Config.OUTPUT_PATH / \"cough_best.pth\"))\n",
    "model_v.load_state_dict(torch.load(Config.OUTPUT_PATH / \"vowel_best.pth\"))\n",
    "model_c.eval()\n",
    "model_v.eval()\n",
    "\n",
    "def get_probs(model, df, img_path):\n",
    "    df_temp = df.rename(columns={'candidateID': 'id'})\n",
    "    ds = SpectrogramDataset(df_temp, img_path, transform=get_transforms('valid'), is_test=True)\n",
    "    loader = DataLoader(ds, batch_size=32, shuffle=False, num_workers=2, collate_fn=collate_fn)\n",
    "    \n",
    "    probs_map = {}\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            if batch is None: \n",
    "                continue\n",
    "            imgs, ids = batch\n",
    "            imgs = imgs.to(Config.DEVICE)\n",
    "            with autocast():\n",
    "                logits, _ = model(imgs)\n",
    "                probs = F.softmax(logits, dim=1).cpu().numpy()\n",
    "            \n",
    "            for i, uid in enumerate(ids):\n",
    "                probs_map[uid] = probs[i]\n",
    "    return probs_map\n",
    "\n",
    "probs_c = get_probs(model_c, test_df, Config.COUGH_PATH)\n",
    "probs_v = get_probs(model_v, test_df, Config.VOWEL_PATH)\n",
    "\n",
    "print(f\"Cough predictions: {len(probs_c)} | Vowel predictions: {len(probs_v)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe0b148",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = []\n",
    "missing_count = 0\n",
    "\n",
    "for idx, row in test_df.iterrows():\n",
    "    uid = row['candidateID']\n",
    "    p_c = probs_c.get(uid)\n",
    "    p_v = probs_v.get(uid)\n",
    "    \n",
    "    if p_c is not None and p_v is not None:\n",
    "        final_prob = (p_c + p_v) / 2\n",
    "        pred = int(np.argmax(final_prob))\n",
    "    elif p_c is not None:\n",
    "        pred = int(np.argmax(p_c))\n",
    "    elif p_v is not None:\n",
    "        pred = int(np.argmax(p_v))\n",
    "    else:\n",
    "        pred = Config.FALLBACK_CLASS\n",
    "        missing_count += 1\n",
    "        \n",
    "    final_results.append({'candidateID': uid, 'disease': pred})\n",
    "\n",
    "submission = pd.DataFrame(final_results)\n",
    "submission = submission[['candidateID', 'disease']]\n",
    "\n",
    "print(f\"Total: {len(submission)} | Missing: {missing_count}\")\n",
    "print(f\"\\nPrediction distribution:\\n{submission['disease'].value_counts().sort_index()}\")\n",
    "\n",
    "submission.to_csv(Config.OUTPUT_PATH / \"submission.csv\", index=False)\n",
    "submission.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
