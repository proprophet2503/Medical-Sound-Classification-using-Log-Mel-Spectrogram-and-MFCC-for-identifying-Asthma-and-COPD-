{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f19cf90",
   "metadata": {},
   "source": [
    "## Installation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7eb48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \n",
    "                      \"protobuf<=3.20.3\", \"numpy<2.0.0\", \"librosa\", \n",
    "                      \"optuna\", \"tensorflow-hub\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742b9402",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f44d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"\n",
    "\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff197b37",
   "metadata": {},
   "source": [
    "## Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998ba620",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    BASE_DIR = Path(\"/kaggle/input/logmel-cough-vowel\")\n",
    "    DATA_PATH = BASE_DIR / \"dataclean_COUGH_VOWEL\"\n",
    "    CSV_PATH = BASE_DIR / \"logMelDataset\"\n",
    "    \n",
    "    COUGH_PATH = DATA_PATH / \"dataclean_cough_1\"\n",
    "    VOWEL_PATH = DATA_PATH / \"dataclean_vowel_1\"\n",
    "    TRAIN_CSV = CSV_PATH / \"train.csv\"\n",
    "    TEST_CSV = CSV_PATH / \"test.csv\"\n",
    "    OUTPUT_PATH = Path(\"/kaggle/working\")\n",
    "    \n",
    "    YAMNET_URL = \"https://tfhub.dev/google/yamnet/1\"\n",
    "    YAMNET_SAMPLE_RATE = 16000\n",
    "    YAMNET_EMBEDDING_DIM = 1024\n",
    "    \n",
    "    NUM_CLASSES = 3\n",
    "    FALLBACK_CLASS = 1\n",
    "    \n",
    "    WARMUP_EPOCHS = 5\n",
    "    FINETUNE_EPOCHS = 20\n",
    "    LR_REDUCTION_FACTOR = 5\n",
    "    \n",
    "    NUM_WORKERS = 0\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    SEED = 42\n",
    "    \n",
    "    N_TRIALS = 10\n",
    "    OPTUNA_WARMUP_EPOCHS = 3\n",
    "    OPTUNA_FINETUNE_EPOCHS = 5\n",
    "\n",
    "    @staticmethod\n",
    "    def create_output_dir():\n",
    "        Config.OUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "set_seed(Config.SEED)\n",
    "Config.create_output_dir()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2f268a",
   "metadata": {},
   "source": [
    "## YAMNet Model & Embedding Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fd4d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.set_visible_devices([], 'GPU')\n",
    "yamnet_model = hub.load(Config.YAMNET_URL)\n",
    "\n",
    "def load_wav_16k_mono(filepath):\n",
    "    try:\n",
    "        waveform, sr = librosa.load(filepath, sr=None, mono=True)\n",
    "        if sr != Config.YAMNET_SAMPLE_RATE:\n",
    "            waveform = librosa.resample(waveform, orig_sr=sr, target_sr=Config.YAMNET_SAMPLE_RATE)\n",
    "        \n",
    "        waveform = waveform.astype(np.float32)\n",
    "        if np.abs(waveform).max() > 0:\n",
    "            waveform = waveform / np.abs(waveform).max()\n",
    "        return waveform\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def extract_yamnet_embeddings(waveform):\n",
    "    if waveform is None: \n",
    "        return None\n",
    "    try:\n",
    "        waveform_tf = tf.constant(waveform, dtype=tf.float32)\n",
    "        _, embeddings, _ = yamnet_model(waveform_tf)\n",
    "        embedding = tf.reduce_mean(embeddings, axis=0).numpy()\n",
    "        return embedding\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def extract_all_embeddings(df, audio_dir, audio_type='cough'):\n",
    "    audio_filename = f'{audio_type}.wav'\n",
    "    embeddings_dict = {}\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        uid = row['id'] if 'id' in row else row['candidateID']\n",
    "        audio_path = audio_dir / uid / audio_filename\n",
    "        \n",
    "        if not audio_path.exists():\n",
    "            continue\n",
    "        \n",
    "        waveform = load_wav_16k_mono(str(audio_path))\n",
    "        if waveform is None:\n",
    "            continue\n",
    "        \n",
    "        embedding = extract_yamnet_embeddings(waveform)\n",
    "        if embedding is None:\n",
    "            continue\n",
    "        \n",
    "        embeddings_dict[uid] = embedding\n",
    "    \n",
    "    return embeddings_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8561d559",
   "metadata": {},
   "source": [
    "## Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1143bd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, df, embeddings_dict, is_test=False):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.embeddings_dict = embeddings_dict\n",
    "        self.is_test = is_test\n",
    "        \n",
    "        self.valid_indices = []\n",
    "        for idx, row in self.df.iterrows():\n",
    "            uid = row['id'] if 'id' in row else row['candidateID']\n",
    "            if uid in self.embeddings_dict:\n",
    "                self.valid_indices.append(idx)\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.valid_indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        real_idx = self.valid_indices[idx]\n",
    "        row = self.df.iloc[real_idx]\n",
    "        uid = row['id'] if 'id' in row else row['candidateID']\n",
    "        \n",
    "        embedding = torch.from_numpy(self.embeddings_dict[uid]).float()\n",
    "        \n",
    "        if self.is_test:\n",
    "            return embedding, uid\n",
    "        else:\n",
    "            label = int(row['label'])\n",
    "            return embedding, label, uid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841e8b0f",
   "metadata": {},
   "source": [
    "## Model Architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0619940",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YAMNetClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim=Config.YAMNET_EMBEDDING_DIM, num_classes=Config.NUM_CLASSES):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        logits = self.classifier(x)\n",
    "        return logits, x\n",
    "    \n",
    "    def freeze_early_layers(self):\n",
    "        for param in self.classifier[0].parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def unfreeze_all(self):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    def get_trainable_params(self):\n",
    "        return [p for p in self.parameters() if p.requires_grad]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b861db6d",
   "metadata": {},
   "source": [
    "## Training Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837784f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, scaler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "    \n",
    "    for batch in loader:\n",
    "        embeddings, labels, _ = batch\n",
    "        embeddings, labels = embeddings.to(Config.DEVICE), labels.to(Config.DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            logits, _ = model(embeddings)\n",
    "            loss = criterion(logits, labels)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    if len(all_preds) == 0: \n",
    "        return 0, 0\n",
    "    return total_loss / len(loader), f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "\n",
    "def validate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            embeddings, labels, _ = batch\n",
    "            embeddings, labels = embeddings.to(Config.DEVICE), labels.to(Config.DEVICE)\n",
    "            \n",
    "            with autocast():\n",
    "                logits, _ = model(embeddings)\n",
    "                loss = criterion(logits, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    if len(all_preds) == 0: \n",
    "        return 0, 0, [], []\n",
    "    return total_loss / len(loader), f1_score(all_labels, all_preds, average='macro', zero_division=0), all_preds, all_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4915d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pipeline(model, train_loader, val_loader, params, save_name, class_weights_tensor,\n",
    "                   warmup_epochs=Config.WARMUP_EPOCHS, finetune_epochs=Config.FINETUNE_EPOCHS):\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor, label_smoothing=0.1)\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    best_f1 = 0\n",
    "    history = {'train_loss': [], 'val_f1': []}\n",
    "    base_lr = params['lr']\n",
    "    \n",
    "    model.freeze_early_layers()\n",
    "    optimizer = AdamW(model.get_trainable_params(), lr=base_lr, weight_decay=1e-2)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=warmup_epochs)\n",
    "    \n",
    "    for epoch in range(warmup_epochs):\n",
    "        t_loss, t_f1 = train_epoch(model, train_loader, criterion, optimizer, scaler)\n",
    "        v_loss, v_f1, _, _ = validate(model, val_loader, criterion)\n",
    "        \n",
    "        history['train_loss'].append(t_loss)\n",
    "        history['val_f1'].append(v_f1)\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"[{save_name.upper()}] Warmup Epoch {epoch+1}/{warmup_epochs} | Loss: {t_loss:.4f} | Val F1: {v_f1:.4f}\")\n",
    "        \n",
    "        if v_f1 > best_f1:\n",
    "            best_f1 = v_f1\n",
    "            torch.save(model.state_dict(), Config.OUTPUT_PATH / f\"{save_name}_best.pth\")\n",
    "        \n",
    "        scheduler.step()\n",
    "    \n",
    "    model.unfreeze_all()\n",
    "    ft_lr = base_lr / Config.LR_REDUCTION_FACTOR\n",
    "    optimizer = AdamW(model.parameters(), lr=ft_lr, weight_decay=1e-2)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=finetune_epochs)\n",
    "    \n",
    "    for epoch in range(finetune_epochs):\n",
    "        t_loss, t_f1 = train_epoch(model, train_loader, criterion, optimizer, scaler)\n",
    "        v_loss, v_f1, _, _ = validate(model, val_loader, criterion)\n",
    "        \n",
    "        history['train_loss'].append(t_loss)\n",
    "        history['val_f1'].append(v_f1)\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"[{save_name.upper()}] Finetune Epoch {epoch+1}/{finetune_epochs} | Loss: {t_loss:.4f} | Val F1: {v_f1:.4f}\")\n",
    "        \n",
    "        if v_f1 > best_f1:\n",
    "            best_f1 = v_f1\n",
    "            torch.save(model.state_dict(), Config.OUTPUT_PATH / f\"{save_name}_best.pth\")\n",
    "            \n",
    "        scheduler.step()\n",
    "        \n",
    "    return history, best_f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf5c2f6",
   "metadata": {},
   "source": [
    "## Optuna Hyperparameter Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5dac75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, train_df, embeddings_dict, class_weights_tensor):\n",
    "    lr = trial.suggest_float('lr', 1e-4, 5e-3, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32])\n",
    "    \n",
    "    tr_df, val_df = train_test_split(train_df, test_size=0.2, stratify=train_df['label'], random_state=Config.SEED)\n",
    "    \n",
    "    tr_loader = DataLoader(EmbeddingDataset(tr_df, embeddings_dict), batch_size=batch_size, \n",
    "                           shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(EmbeddingDataset(val_df, embeddings_dict), batch_size=batch_size, \n",
    "                           shuffle=False, num_workers=0)\n",
    "    \n",
    "    model = YAMNetClassifier().to(Config.DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor, label_smoothing=0.1)\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    best_temp_f1 = 0\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(Config.OPTUNA_FINETUNE_EPOCHS):\n",
    "        train_epoch(model, tr_loader, criterion, optimizer, scaler)\n",
    "        _, f1, _, _ = validate(model, val_loader, criterion)\n",
    "        \n",
    "        best_temp_f1 = max(best_temp_f1, f1)\n",
    "        trial.report(f1, epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "            \n",
    "    return best_temp_f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e609e86",
   "metadata": {},
   "source": [
    "## Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49891d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(Config.TRAIN_CSV)\n",
    "test_df = pd.read_csv(Config.TEST_CSV)\n",
    "train_df = train_df.rename(columns={'disease': 'label', 'candidateID': 'id'})\n",
    "test_df = test_df.rename(columns={'candidateID': 'id'})\n",
    "\n",
    "y_train = train_df['label'].values\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(Config.DEVICE)\n",
    "\n",
    "train_df['has_cough'] = train_df['id'].isin(set(p.name for p in Config.COUGH_PATH.glob(\"*\")))\n",
    "train_df['has_vowel'] = train_df['id'].isin(set(p.name for p in Config.VOWEL_PATH.glob(\"*\")))\n",
    "\n",
    "print(f\"Train: {len(train_df)} | Cough: {train_df['has_cough'].sum()} | Vowel: {train_df['has_vowel'].sum()}\")\n",
    "print(f\"Test: {len(test_df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6a1e40",
   "metadata": {},
   "source": [
    "## Extract Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe24e5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c_train = train_df[train_df['has_cough']].copy()\n",
    "cough_train_emb = extract_all_embeddings(df_c_train, Config.COUGH_PATH, 'cough')\n",
    "cough_test_emb = extract_all_embeddings(test_df, Config.COUGH_PATH, 'cough')\n",
    "\n",
    "df_v_train = train_df[train_df['has_vowel']].copy()\n",
    "vowel_train_emb = extract_all_embeddings(df_v_train, Config.VOWEL_PATH, 'vowel')\n",
    "vowel_test_emb = extract_all_embeddings(test_df, Config.VOWEL_PATH, 'vowel')\n",
    "\n",
    "del yamnet_model\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ad0a0e",
   "metadata": {},
   "source": [
    "## Train Cough Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d626099",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c_filtered = df_c_train[df_c_train['id'].isin(cough_train_emb.keys())].copy()\n",
    "\n",
    "study_c = optuna.create_study(direction='maximize', pruner=optuna.pruners.MedianPruner())\n",
    "study_c.optimize(lambda t: objective(t, df_c_filtered, cough_train_emb, class_weights_tensor), \n",
    "                 n_trials=Config.N_TRIALS, show_progress_bar=True)\n",
    "params_c = study_c.best_params\n",
    "\n",
    "print(f\"Best Params: {params_c} | F1: {study_c.best_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5301c7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_c, val_c = train_test_split(df_c_filtered, test_size=0.2, stratify=df_c_filtered['label'], random_state=Config.SEED)\n",
    "model_c = YAMNetClassifier().to(Config.DEVICE)\n",
    "\n",
    "hist_c, best_f1_c = train_pipeline(\n",
    "    model_c,\n",
    "    DataLoader(EmbeddingDataset(tr_c, cough_train_emb), batch_size=params_c['batch_size'], shuffle=True),\n",
    "    DataLoader(EmbeddingDataset(val_c, cough_train_emb), batch_size=params_c['batch_size']),\n",
    "    params_c, \"cough\", class_weights_tensor\n",
    ")\n",
    "\n",
    "print(f\"Cough Model Best F1: {best_f1_c:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe815896",
   "metadata": {},
   "source": [
    "### Cough Model Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d957bd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_c.load_state_dict(torch.load(Config.OUTPUT_PATH / \"cough_best.pth\"))\n",
    "model_c.eval()\n",
    "\n",
    "val_c_loader = DataLoader(\n",
    "    EmbeddingDataset(val_c, cough_train_emb), \n",
    "    batch_size=32, shuffle=False, num_workers=0\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor, label_smoothing=0.1)\n",
    "val_loss, val_f1, val_preds, val_labels = validate(model_c, val_c_loader, criterion)\n",
    "\n",
    "val_acc = accuracy_score(val_labels, val_preds)\n",
    "class_f1 = f1_score(val_labels, val_preds, average=None, zero_division=0)\n",
    "\n",
    "print(f\"Accuracy: {val_acc:.4f} | F1 Macro: {val_f1:.4f}\")\n",
    "print(f\"Per-Class F1: {class_f1}\")\n",
    "print(f\"\\n{classification_report(val_labels, val_preds, target_names=['Class 0', 'Class 1', 'Class 2'], zero_division=0)}\")\n",
    "\n",
    "cm_c = confusion_matrix(val_labels, val_preds)\n",
    "fig_cm_c, ax_cm_c = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(cm_c, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Class 0', 'Class 1', 'Class 2'],\n",
    "            yticklabels=['Class 0', 'Class 1', 'Class 2'],\n",
    "            ax=ax_cm_c, cbar_kws={'label': 'Count'})\n",
    "ax_cm_c.set_xlabel('Predicted', fontsize=12, fontweight='bold')\n",
    "ax_cm_c.set_ylabel('True', fontsize=12, fontweight='bold')\n",
    "ax_cm_c.set_title('Cough Model - Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(Config.OUTPUT_PATH / \"cough_confusion_matrix.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d45ceb",
   "metadata": {},
   "source": [
    "## Train Vowel Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f33d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_v_filtered = df_v_train[df_v_train['id'].isin(vowel_train_emb.keys())].copy()\n",
    "\n",
    "study_v = optuna.create_study(direction='maximize', pruner=optuna.pruners.MedianPruner())\n",
    "study_v.optimize(lambda t: objective(t, df_v_filtered, vowel_train_emb, class_weights_tensor),\n",
    "                 n_trials=Config.N_TRIALS, show_progress_bar=True)\n",
    "params_v = study_v.best_params\n",
    "\n",
    "print(f\"Best Params: {params_v} | F1: {study_v.best_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd05a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_v, val_v = train_test_split(df_v_filtered, test_size=0.2, stratify=df_v_filtered['label'], random_state=Config.SEED)\n",
    "model_v = YAMNetClassifier().to(Config.DEVICE)\n",
    "\n",
    "hist_v, best_f1_v = train_pipeline(\n",
    "    model_v,\n",
    "    DataLoader(EmbeddingDataset(tr_v, vowel_train_emb), batch_size=params_v['batch_size'], shuffle=True),\n",
    "    DataLoader(EmbeddingDataset(val_v, vowel_train_emb), batch_size=params_v['batch_size']),\n",
    "    params_v, \"vowel\", class_weights_tensor\n",
    ")\n",
    "\n",
    "print(f\"Vowel Model Best F1: {best_f1_v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f2835c",
   "metadata": {},
   "source": [
    "### Vowel Model Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d405d83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_v.load_state_dict(torch.load(Config.OUTPUT_PATH / \"vowel_best.pth\"))\n",
    "model_v.eval()\n",
    "\n",
    "val_v_loader = DataLoader(\n",
    "    EmbeddingDataset(val_v, vowel_train_emb), \n",
    "    batch_size=32, shuffle=False, num_workers=0\n",
    ")\n",
    "\n",
    "val_loss_v, val_f1_v, val_preds_v, val_labels_v = validate(model_v, val_v_loader, criterion)\n",
    "\n",
    "val_acc_v = accuracy_score(val_labels_v, val_preds_v)\n",
    "class_f1_v = f1_score(val_labels_v, val_preds_v, average=None, zero_division=0)\n",
    "\n",
    "print(f\"Accuracy: {val_acc_v:.4f} | F1 Macro: {val_f1_v:.4f}\")\n",
    "print(f\"Per-Class F1: {class_f1_v}\")\n",
    "print(f\"\\n{classification_report(val_labels_v, val_preds_v, target_names=['Class 0', 'Class 1', 'Class 2'], zero_division=0)}\")\n",
    "\n",
    "cm_v = confusion_matrix(val_labels_v, val_preds_v)\n",
    "fig_cm_v, ax_cm_v = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(cm_v, annot=True, fmt='d', cmap='Greens', \n",
    "            xticklabels=['Class 0', 'Class 1', 'Class 2'],\n",
    "            yticklabels=['Class 0', 'Class 1', 'Class 2'],\n",
    "            ax=ax_cm_v, cbar_kws={'label': 'Count'})\n",
    "ax_cm_v.set_xlabel('Predicted', fontsize=12, fontweight='bold')\n",
    "ax_cm_v.set_ylabel('True', fontsize=12, fontweight='bold')\n",
    "ax_cm_v.set_title('Vowel Model - Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(Config.OUTPUT_PATH / \"vowel_confusion_matrix.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd03100e",
   "metadata": {},
   "source": [
    "## Training Metrics Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00842b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax[0].plot(hist_c['val_f1'], marker='o', label=f'Cough (Best: {best_f1_c:.3f})', linewidth=2)\n",
    "ax[0].plot(hist_v['val_f1'], marker='s', label=f'Vowel (Best: {best_f1_v:.3f})', linewidth=2)\n",
    "ax[0].set_xlabel('Epoch', fontsize=12)\n",
    "ax[0].set_ylabel('Validation F1 Score', fontsize=12)\n",
    "ax[0].set_title('Model Performance', fontsize=14, fontweight='bold')\n",
    "ax[0].legend(fontsize=10)\n",
    "ax[0].grid(True, alpha=0.3)\n",
    "\n",
    "ax[1].plot(hist_c['train_loss'], marker='o', label='Cough', linewidth=2)\n",
    "ax[1].plot(hist_v['train_loss'], marker='s', label='Vowel', linewidth=2)\n",
    "ax[1].set_xlabel('Epoch', fontsize=12)\n",
    "ax[1].set_ylabel('Training Loss', fontsize=12)\n",
    "ax[1].set_title('Training Loss', fontsize=14, fontweight='bold')\n",
    "ax[1].legend(fontsize=10)\n",
    "ax[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(Config.OUTPUT_PATH / \"training_metrics.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75eea61e",
   "metadata": {},
   "source": [
    "## Generate Submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bff9619",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_c.load_state_dict(torch.load(Config.OUTPUT_PATH / \"cough_best.pth\"))\n",
    "model_v.load_state_dict(torch.load(Config.OUTPUT_PATH / \"vowel_best.pth\"))\n",
    "model_c.eval()\n",
    "model_v.eval()\n",
    "\n",
    "def get_probs(model, embeddings_dict):\n",
    "    probs_map = {}\n",
    "    with torch.no_grad():\n",
    "        for uid, emb in embeddings_dict.items():\n",
    "            emb_tensor = torch.from_numpy(emb).float().unsqueeze(0).to(Config.DEVICE)\n",
    "            with autocast():\n",
    "                logits, _ = model(emb_tensor)\n",
    "                probs = F.softmax(logits, dim=1).cpu().numpy()[0]\n",
    "            probs_map[uid] = probs\n",
    "    return probs_map\n",
    "\n",
    "probs_c = get_probs(model_c, cough_test_emb)\n",
    "probs_v = get_probs(model_v, vowel_test_emb)\n",
    "\n",
    "print(f\"Cough predictions: {len(probs_c)} | Vowel predictions: {len(probs_v)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f3a564",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = []\n",
    "missing_count = 0\n",
    "\n",
    "for idx, row in test_df.iterrows():\n",
    "    uid = row['id']\n",
    "    p_c = probs_c.get(uid)\n",
    "    p_v = probs_v.get(uid)\n",
    "    \n",
    "    if p_c is not None and p_v is not None:\n",
    "        final_prob = (p_c + p_v) / 2\n",
    "        pred = int(np.argmax(final_prob))\n",
    "    elif p_c is not None:\n",
    "        pred = int(np.argmax(p_c))\n",
    "    elif p_v is not None:\n",
    "        pred = int(np.argmax(p_v))\n",
    "    else:\n",
    "        pred = Config.FALLBACK_CLASS\n",
    "        missing_count += 1\n",
    "        \n",
    "    final_results.append({'candidateID': uid, 'disease': pred})\n",
    "\n",
    "submission = pd.DataFrame(final_results)\n",
    "submission = submission[['candidateID', 'disease']]\n",
    "\n",
    "print(f\"Total: {len(submission)} | Missing: {missing_count}\")\n",
    "print(f\"\\nPrediction distribution:\\n{submission['disease'].value_counts().sort_index()}\")\n",
    "\n",
    "submission.to_csv(Config.OUTPUT_PATH / \"submission.csv\", index=False)\n",
    "submission.head(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
